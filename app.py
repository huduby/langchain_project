import os
import tempfile

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts  import ChatPromptTemplate

import streamlit as st

from dotenv import load_dotenv # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ëª¨ë“ˆ
load_dotenv()  # ì‹¤í–‰ ì‹œ .env íŒŒì¼ì„ ì°¾ì•„ ë³€ìˆ˜ë“¤ì„ í™˜ê²½ì— ë¡œë“œ

# --------------------------------------------------
# PDF â†’ VectorDB ë³€í™˜ í•¨ìˆ˜ (ìºì‹± ì ìš©)
# --------------------------------------------------
    
# ìºì‹± ì²˜ë¦¬
@st.cache_resource
def build_vector_db(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        pdf_path = tmp_file.name

    loader = PyPDFLoader(pdf_path)
    docs = loader.load()

    os.remove(pdf_path)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(chunks, embeddings)

    return vectordb

# --------------------------------------------------
#  RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
# --------------------------------------------------
def get_response(query, vectorstore,chat_history):
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.0)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) 
    system_prompt = (
        "ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ì„ ê³ ê°ì˜ ëˆˆë†’ì´ì—ì„œ ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” 'AI ë³´í—˜ ê°€ì´ë“œ'ì…ë‹ˆë‹¤."
        "ì•„ë˜ ì œê³µëœ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”."
        "[ì§€ì¹¨ ì‚¬í•­]"
        "1. í†¤ì•¤ë§¤ë„ˆ: ë”±ë”±í•œ ë§íˆ¬ ëŒ€ì‹ , ê³ ê°ì„ ëŒ€í•˜ë“¯ ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë¬¸ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        "2. ì‚¬ì‹¤ ê¸°ë°˜: ë°˜ë“œì‹œ [Context]ì— ëª…ì‹œëœ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì•½ê´€ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "
        "'ì•½ê´€ ë‚´ìš©ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”'ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”."
        "3. ì¶œì²˜ ëª…ì‹œ: ë‹µë³€ì˜ ëì—ëŠ” ë°˜ë“œì‹œ ê·¼ê±°ê°€ ë˜ëŠ” 'ê´€ë ¨ ì¡°í•­(ì œ ëª‡ ì¡°)'ì´ë‚˜ 'í˜ì´ì§€'ë¥¼ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”."
        "4. í‘œí˜„: 'ì•½ê´€ì— ë”°ë¥´ë©´ ~ë¼ê³  ë˜ì–´ ìˆì–´ìš”'ë¼ëŠ” ê°ê´€ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        "5. ëŒ€ìƒ: ì„±ì¸ ëŒ€ìƒì´ì§€ë§Œ ë³´í—˜ì— ëŒ€í•´ì„œëŠ” ì˜ ëª¨ë¥´ëŠ” ì‚¬ëŒì„ ëŒ€ìƒìœ¼ë¡œ ì‰½ê²Œ ì˜ í’€ì–´ì„œ ì„¤ëª…í•´ ì¤˜ì•¼ í•œë‹¤."
        "[Context]: {context}"
    )  
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "{question}")
    ])

    rag_chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )
    response = rag_chain.invoke(query)
    return response.content


# --------------------------------------------------
# Session State ì´ˆê¸°í™”
# --------------------------------------------------
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë³´í—˜ ì•½ê´€ PDFë¥¼ ì—…ë¡œë“œí•´ ì£¼ì‹œë©´ ë¶„ì„í•´ ë“œë¦´ê²Œìš” ğŸ˜Š"}
    ]

# --------------------------------------------------
#  Streamlit ê¸°ë³¸ ì„¤ì •
# --------------------------------------------------
st.set_page_config(
    page_title="AI ë³´í—˜ ì•½ê´€ ë¶„ì„ê¸°",
    page_icon="ğŸ“‘",
    layout="wide"
)

main_title = """
AI ë³´í—˜ ì•½ê´€ ë¶„ì„ê¸°ëŠ”
ë‹¹ì‹ ì´ ê°€ì…í•œ ë³´í—˜ ì•½ê´€(PDF)ì„ ì—…ë¡œë“œí•´ ë‘ê³ ,
ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì§ˆë¬¸í•˜ë©´ ë‹µë³€ì„ ë„ì™€ì£¼ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

ì–´ë ¤ìš´ ë³´í—˜ ìš©ì–´ë¥¼ ê³ ê°ì˜ ëˆˆë†’ì´ì— ë§ì¶”ì–´ ì„¤ëª…í•´ ë“œë¦¬ë©°,
ë³´ì¥ ë‚´ìš©ãƒ»ë©´ì±… ì‚¬í•­ãƒ»ë³´ì¥ í•œë„ ë“± í•µì‹¬ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”.

ì—…ë¡œë“œë§Œ í•´ ë‘ë©´, ë‚˜ë¨¸ì§€ëŠ” AIê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•´ ë“œë¦´ê²Œìš” ğŸ™‚
"""

# --------------------------------------------------
# ë©”ì¸ í™”ë©´
# --------------------------------------------------
st.header("ğŸ” AI ë³´í—˜ ì•½ê´€ Q&A")
st.info(f"ğŸ’¡{main_title}")
st.subheader("ğŸ“„ ì•½ê´€ PDF ì—…ë¡œë“œ")

if st.session_state.vectorstore is None:    
    uploaded_file = st.file_uploader(
        "ë³´í—˜ì•½ê´€ PDFë¥¼ ì˜¬ë ¤ì£¼ì„¸ìš”.",
        type=["pdf"],
        key="upload_pdf"    )
    if uploaded_file and st.button("ì•½ê´€ ë¶„ì„ ì‹œì‘í•˜ê¸°", key="add_policy"):     
        with st.spinner("ì•½ê´€ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)"):
            try:
                st.session_state.vectorstore = build_vector_db(uploaded_file)
                # print(st.session_state.vectorstore)
                policy_name = uploaded_file.name
                st.success(f"'{policy_name}' ì•½ê´€ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ì•½ê´€ìœ¼ë¡œ ì§ˆë¬¸í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”.")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.stop()
else:
    st.info("ì´ë¯¸ ë²¡í„° DBê°€ ìƒì„±ë˜ì–´ ìˆì–´ìš”. ë‹¤ì‹œ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ì´ì „ ëŒ€í™” ì¶œë ¥
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = st.chat_input("ë³´í—˜ ì•½ê´€ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
if query and st.session_state.vectorstore is not None:
    # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    st.session_state.chat_history.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ì•½ê´€ì—ì„œ ë‹µì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            answer = get_response(query, st.session_state.vectorstore,st.session_state.chat_history)
            st.markdown(answer)
            st.session_state.chat_history.append({"role": "assistant", "content": answer})